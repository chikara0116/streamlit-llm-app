from dotenv import load_dotenv

load_dotenv()
import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)

# Define the function to query the LLM based on user input and expert selection
def query_expert(input_text: str, expert_role: str) -> str:
    """
    Sends the input text and expert role to the LLM and returns the response.
    :param input_text: Text provided by the user.
    :param expert_role: Selected expert type (e.g., 'Technology', 'Business').
    :return: LLM-generated response.
    """
    # Define system messages for different experts
    system_prompts = {
        "Technology Expert": "You are a highly knowledgeable technology expert. Provide detailed technical insights.",
        "Business Strategist": "You are an experienced business strategist. Offer practical business recommendations."
    }
    
    # Select the appropriate system message
    system_message = system_prompts.get(expert_role, "You are a helpful assistant.")

    # Build the chat prompt template
    chat_prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(system_message),
        HumanMessagePromptTemplate.from_template("User said: {user_input}")
    ])

    # Initialize the chat model (adjust temperature or model name as needed)
    chat = ChatOpenAI(temperature=0.7)

    # Format messages and send to model
    messages = chat_prompt.format_messages(user_input=input_text)
    response = chat(messages)
    return response.content

# Streamlit UI setup
def main():
    st.title("LLM Expert Assistant Web App")
    
    # Display overview and instructions
    st.markdown(
        """
        ### Webã‚¢ãƒ—ãƒªæ¦‚è¦
        ã“ã®ã‚¢ãƒ—ãƒªã§ã¯ã€å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã€ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§å°‚é–€å®¶ã‚’é¸æŠã™ã‚‹ã¨ã€
        é¸æŠã—ãŸå°‚é–€å®¶ã®è¦–ç‚¹ã§LLMãŒå›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
        - ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚
        - å°‚é–€å®¶ã®é¸æŠè‚¢ã«å¿œã˜ã¦å›ç­”ã®ã‚¹ã‚¿ã‚¤ãƒ«ãŒå¤‰ã‚ã‚Šã¾ã™ã€‚
        """
    )

    # Input form and expert selection
    user_input = st.text_area("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ", height=150)
    expert_option = st.radio(
        "å°‚é–€å®¶ã‚’é¸æŠ", 
        ("Technology Expert", "Business Strategist")
    )

    # When the user clicks the button, call the LLM function
    if st.button("é€ä¿¡"):
        if not user_input.strip():
            st.warning("ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­... ğŸ§ "):
                llm_response = query_expert(user_input, expert_option)
            st.subheader("LLMã®å›ç­”")
            st.write(llm_response)

if __name__ == "__main__":
    main()
# This code is a Streamlit web application that allows users to interact with a language model (LLM)

# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
# pip install streamlit langchain openai

import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)

# Define the function to query the LLM based on user input and expert selection
def query_expert(input_text: str, expert_role: str) -> str:
    """
    Sends the input text and expert role to the LLM and returns the response.
    :param input_text: Text provided by the user.
    :param expert_role: Selected expert type (e.g., 'Technology', 'Business').
    :return: LLM-generated response.
    """
    # Define system messages for different experts
    system_prompts = {
        "Technology Expert": "You are a highly knowledgeable technology expert. Provide detailed technical insights.",
        "Business Strategist": "You are an experienced business strategist. Offer practical business recommendations."
    }
    
    # Select the appropriate system message
    system_message = system_prompts.get(expert_role, "You are a helpful assistant.")

    # Build the chat prompt template
    chat_prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(system_message),
        HumanMessagePromptTemplate.from_template("User said: {user_input}")
    ])

    # Initialize the chat model (adjust temperature or model name as needed)
    chat = ChatOpenAI(temperature=0.7)

    # Format messages and send to model
    messages = chat_prompt.format_messages(user_input=input_text)
    response = chat(messages)
    return response.content

# Streamlit UI setup
def main():
    st.title("LLM Expert Assistant Web App")
    
    # Display overview and instructions
    st.markdown(
        """
        ### Webã‚¢ãƒ—ãƒªæ¦‚è¦
        ã“ã®ã‚¢ãƒ—ãƒªã§ã¯ã€å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã€ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§å°‚é–€å®¶ã‚’é¸æŠã™ã‚‹ã¨ã€
        é¸æŠã—ãŸå°‚é–€å®¶ã®è¦–ç‚¹ã§LLMãŒå›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
        - ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚
        - å°‚é–€å®¶ã®é¸æŠè‚¢ã«å¿œã˜ã¦å›ç­”ã®ã‚¹ã‚¿ã‚¤ãƒ«ãŒå¤‰ã‚ã‚Šã¾ã™ã€‚
        """
    )

    # Input form and expert selection
    user_input = st.text_area("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ", height=150)
    expert_option = st.radio(
        "å°‚é–€å®¶ã‚’é¸æŠ", 
        ("Technology Expert", "Business Strategist")
    )

    # When the user clicks the button, call the LLM function
    if st.button("é€ä¿¡"):
        if not user_input.strip():
            st.warning("ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­... ğŸ§ "):
                llm_response = query_expert(user_input, expert_option)
            st.subheader("LLMã®å›ç­”")
            st.write(llm_response)

if __name__ == "__main__":
    main()

# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
# pip install streamlit langchain openai langchain_community

import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)

# Define the function to query the LLM based on user input and expert selection
def query_expert(input_text: str, expert_role: str) -> str:
    """
    Sends the input text and expert role to the LLM and returns the response.
    :param input_text: Text provided by the user.
    :param expert_role: Selected expert type (e.g., 'Technology', 'Business').
    :return: LLM-generated response.
    """
    # Define system messages for different experts
    system_prompts = {
        "Technology Expert": "You are a highly knowledgeable technology expert. Provide detailed technical insights.",
        "Business Strategist": "You are an experienced business strategist. Offer practical business recommendations."
    }
    
    # Select the appropriate system message
    system_message = system_prompts.get(expert_role, "You are a helpful assistant.")

    # Build the chat prompt template
    chat_prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(system_message),
        HumanMessagePromptTemplate.from_template("User said: {user_input}")
    ])

    # Initialize the chat model (adjust temperature or model name as needed)
    chat = ChatOpenAI(temperature=0.7)

    # Format messages and send to model
    messages = chat_prompt.format_messages(user_input=input_text)
    response = chat(messages)
    return response.content

# Streamlit UI setup
def main():
    st.title("LLM Expert Assistant Web App")
    
    # Display overview and instructions
    st.markdown(
        """
        ### Webã‚¢ãƒ—ãƒªæ¦‚è¦
        ã“ã®ã‚¢ãƒ—ãƒªã§ã¯ã€å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã€ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§å°‚é–€å®¶ã‚’é¸æŠã™ã‚‹ã¨ã€
        é¸æŠã—ãŸå°‚é–€å®¶ã®è¦–ç‚¹ã§LLMãŒå›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
        - ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚
        - å°‚é–€å®¶ã®é¸æŠè‚¢ã«å¿œã˜ã¦å›ç­”ã®ã‚¹ã‚¿ã‚¤ãƒ«ãŒå¤‰ã‚ã‚Šã¾ã™ã€‚
        """
    )

    # Input form and expert selection
    user_input = st.text_area("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ", height=150)
    expert_option = st.radio(
        "å°‚é–€å®¶ã‚’é¸æŠ", 
        ("Technology Expert", "Business Strategist")
    )

    # When the user clicks the button, call the LLM function
    if st.button("é€ä¿¡"):
        if not user_input.strip():
            st.warning("ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­... ğŸ§ "):
                llm_response = query_expert(user_input, expert_option)
            st.subheader("LLMã®å›ç­”")
            st.write(llm_response)

if __name__ == "__main__":
    main()
